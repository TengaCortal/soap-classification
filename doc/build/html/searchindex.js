Search.setIndex({"docnames": ["index", "modules", "soap_classification", "soap_classification.data_cleaning", "soap_classification.preprocessing"], "filenames": ["index.rst", "modules.rst", "soap_classification.rst", "soap_classification.data_cleaning.rst", "soap_classification.preprocessing.rst"], "titles": ["Welcome to SOAP classification\u2019s documentation!", "soap_classification", "soap_classification package", "soap_classification.data_cleaning package", "soap_classification.preprocessing package"], "terms": {"soap_classif": 0, "packag": [0, 1], "subpackag": [0, 1], "modul": [0, 1], "index": 0, "search": [0, 3], "page": 0, "data_clean": [1, 2], "submodul": [1, 2], "extract_english_vocab": [1, 2], "only_soap": [1, 2], "regex": [1, 2], "remove_undesir": [1, 2], "retrieve_classified_soap": [1, 2], "total_soap": [1, 2], "unify_syntax": [1, 2], "content": 1, "preprocess": [1, 2], "hybrid_token": [1, 2], "preprocess_text": [1, 2], "tokenization_mod": [1, 2], "extract_english_word": [2, 3], "clean_text": [2, 3], "remove_rows_with_pattern": [2, 3], "replace_sub": [2, 3], "basictoken": [2, 4], "token": [2, 4], "fulltoken": [2, 4], "convert_ids_to_token": [2, 4], "convert_tokens_to_id": [2, 4], "fulltokenizerformecab": [2, 4], "mecabtoken": [2, 4], "wordpiecetoken": [2, 4], "convert_by_vocab": [2, 4], "convert_to_unicod": [2, 4], "load_vocab": [2, 4], "printable_text": [2, 4], "validate_case_matches_checkpoint": [2, 4], "whitespace_token": [2, 4], "text": [3, 4], "extract": 3, "english": 3, "word": [3, 4], "from": 3, "given": [3, 4], "paramet": [3, 4], "str": [3, 4], "The": [3, 4], "input": [3, 4], "contain": 3, "return": [3, 4], "A": [3, 4], "list": [3, 4], "type": [3, 4], "clean": [3, 4], "remov": 3, "unnecessari": 3, "charact": 3, "specifi": 3, "pattern": 3, "after": [3, 4], "datafram": 3, "row": 3, "panda": 3, "which": 3, "within": 3, "modifi": 3, "replac": 3, "certain": 3, "substr": 3, "predefin": 3, "abbrevi": 3, "process": 3, "substitut": 3, "appli": [3, 4], "variou": 4, "normal": 4, "format": 4, "step": 4, "class": 4, "do_lower_cas": 4, "fals": 4, "base": 4, "object": 4, "run": 4, "basic": 4, "punctuat": 4, "split": 4, "lower": 4, "case": 4, "etc": 4, "piec": 4, "vocab_fil": 4, "true": 4, "end": 4, "tokenzi": 4, "id": 4, "sub_token": 4, "mecab_ipadic_neologd": 4, "mecab_j_med": 4, "anonymize_person_nam": 4, "name_token": 4, "\uff4e": 4, "vocab": 4, "unk_token": 4, "unk": 4, "max_input_chars_per_word": 4, "200": 4, "wordpiec": 4, "its": 4, "thi": 4, "us": 4, "greedi": 4, "longest": 4, "match": 4, "first": 4, "algorithm": 4, "perform": 4, "vocabulari": 4, "For": 4, "exampl": 4, "unaff": 4, "output": 4, "un": 4, "aff": 4, "abl": 4, "singl": 4, "whitespac": 4, "separ": 4, "should": 4, "have": 4, "alreadi": 4, "been": 4, "pass": 4, "through": 4, "item": 4, "convert": 4, "sequenc": 4, "inv_vocab": 4, "unicod": 4, "": 4, "assum": 4, "utf": 4, "8": 4, "load": 4, "file": 4, "dictionari": 4, "encod": 4, "wai": 4, "suitabl": 4, "print": 4, "tf": 4, "log": 4, "init_checkpoint": 4, "check": 4, "whether": 4, "config": 4, "i": 4, "consist": 4, "checkpoint": 4, "name": 4}, "objects": {"": [[2, 0, 0, "-", "soap_classification"]], "soap_classification": [[3, 0, 0, "-", "data_cleaning"], [4, 0, 0, "-", "preprocessing"]], "soap_classification.data_cleaning": [[3, 0, 0, "-", "extract_english_vocab"], [3, 0, 0, "-", "only_soap"], [3, 0, 0, "-", "regex"], [3, 0, 0, "-", "remove_undesired"], [3, 0, 0, "-", "retrieve_classified_soaps"], [3, 0, 0, "-", "total_soap"], [3, 0, 0, "-", "unify_syntax"]], "soap_classification.data_cleaning.extract_english_vocab": [[3, 1, 1, "", "extract_english_words"]], "soap_classification.data_cleaning.remove_undesired": [[3, 1, 1, "", "clean_text"], [3, 1, 1, "", "remove_rows_with_pattern"]], "soap_classification.data_cleaning.unify_syntax": [[3, 1, 1, "", "replace_sub"]], "soap_classification.preprocessing": [[4, 0, 0, "-", "preprocess_text"], [4, 0, 0, "-", "tokenization_mod"]], "soap_classification.preprocessing.preprocess_text": [[4, 1, 1, "", "preprocess"]], "soap_classification.preprocessing.tokenization_mod": [[4, 2, 1, "", "BasicTokenizer"], [4, 2, 1, "", "FullTokenizer"], [4, 2, 1, "", "FullTokenizerForMecab"], [4, 2, 1, "", "MecabTokenizer"], [4, 2, 1, "", "WordpieceTokenizer"], [4, 1, 1, "", "convert_by_vocab"], [4, 1, 1, "", "convert_ids_to_tokens"], [4, 1, 1, "", "convert_to_unicode"], [4, 1, 1, "", "convert_tokens_to_ids"], [4, 1, 1, "", "load_vocab"], [4, 1, 1, "", "printable_text"], [4, 1, 1, "", "validate_case_matches_checkpoint"], [4, 1, 1, "", "whitespace_tokenize"]], "soap_classification.preprocessing.tokenization_mod.BasicTokenizer": [[4, 3, 1, "", "tokenize"]], "soap_classification.preprocessing.tokenization_mod.FullTokenizer": [[4, 3, 1, "", "convert_ids_to_tokens"], [4, 3, 1, "", "convert_tokens_to_ids"], [4, 3, 1, "", "tokenize"]], "soap_classification.preprocessing.tokenization_mod.FullTokenizerForMecab": [[4, 3, 1, "", "convert_ids_to_tokens"], [4, 3, 1, "", "convert_tokens_to_ids"], [4, 3, 1, "", "tokenize"]], "soap_classification.preprocessing.tokenization_mod.MecabTokenizer": [[4, 3, 1, "", "tokenize"]], "soap_classification.preprocessing.tokenization_mod.WordpieceTokenizer": [[4, 3, 1, "", "tokenize"]]}, "objtypes": {"0": "py:module", "1": "py:function", "2": "py:class", "3": "py:method"}, "objnames": {"0": ["py", "module", "Python module"], "1": ["py", "function", "Python function"], "2": ["py", "class", "Python class"], "3": ["py", "method", "Python method"]}, "titleterms": {"welcom": 0, "soap": 0, "classif": 0, "": 0, "document": 0, "content": [0, 2, 3, 4], "indic": 0, "tabl": 0, "soap_classif": [1, 2, 3, 4], "packag": [2, 3, 4], "subpackag": 2, "modul": [2, 3, 4], "data_clean": 3, "submodul": [3, 4], "extract_english_vocab": 3, "only_soap": 3, "regex": 3, "remove_undesir": 3, "retrieve_classified_soap": 3, "total_soap": 3, "unify_syntax": 3, "preprocess": 4, "hybrid_token": 4, "preprocess_text": 4, "tokenization_mod": 4}, "envversion": {"sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx": 60}, "alltitles": {"Welcome to SOAP classification\u2019s documentation!": [[0, "welcome-to-soap-classification-s-documentation"]], "Contents:": [[0, null]], "Indices and tables": [[0, "indices-and-tables"]], "soap_classification": [[1, "soap-classification"]], "soap_classification package": [[2, "soap-classification-package"]], "Subpackages": [[2, "subpackages"]], "Module contents": [[2, "module-soap_classification"], [3, "module-soap_classification.data_cleaning"], [4, "module-soap_classification.preprocessing"]], "soap_classification.data_cleaning package": [[3, "soap-classification-data-cleaning-package"]], "Submodules": [[3, "submodules"], [4, "submodules"]], "soap_classification.data_cleaning.extract_english_vocab module": [[3, "module-soap_classification.data_cleaning.extract_english_vocab"]], "soap_classification.data_cleaning.only_soap module": [[3, "module-soap_classification.data_cleaning.only_soap"]], "soap_classification.data_cleaning.regex module": [[3, "module-soap_classification.data_cleaning.regex"]], "soap_classification.data_cleaning.remove_undesired module": [[3, "module-soap_classification.data_cleaning.remove_undesired"]], "soap_classification.data_cleaning.retrieve_classified_soaps module": [[3, "module-soap_classification.data_cleaning.retrieve_classified_soaps"]], "soap_classification.data_cleaning.total_soap module": [[3, "module-soap_classification.data_cleaning.total_soap"]], "soap_classification.data_cleaning.unify_syntax module": [[3, "module-soap_classification.data_cleaning.unify_syntax"]], "soap_classification.preprocessing package": [[4, "soap-classification-preprocessing-package"]], "soap_classification.preprocessing.hybrid_tokenization module": [[4, "soap-classification-preprocessing-hybrid-tokenization-module"]], "soap_classification.preprocessing.preprocess_text module": [[4, "module-soap_classification.preprocessing.preprocess_text"]], "soap_classification.preprocessing.tokenization_mod module": [[4, "module-soap_classification.preprocessing.tokenization_mod"]]}, "indexentries": {"module": [[2, "module-soap_classification"], [3, "module-soap_classification.data_cleaning"], [3, "module-soap_classification.data_cleaning.extract_english_vocab"], [3, "module-soap_classification.data_cleaning.only_soap"], [3, "module-soap_classification.data_cleaning.regex"], [3, "module-soap_classification.data_cleaning.remove_undesired"], [3, "module-soap_classification.data_cleaning.retrieve_classified_soaps"], [3, "module-soap_classification.data_cleaning.total_soap"], [3, "module-soap_classification.data_cleaning.unify_syntax"], [4, "module-soap_classification.preprocessing"], [4, "module-soap_classification.preprocessing.preprocess_text"], [4, "module-soap_classification.preprocessing.tokenization_mod"]], "soap_classification": [[2, "module-soap_classification"]], "clean_text() (in module soap_classification.data_cleaning.remove_undesired)": [[3, "soap_classification.data_cleaning.remove_undesired.clean_text"]], "extract_english_words() (in module soap_classification.data_cleaning.extract_english_vocab)": [[3, "soap_classification.data_cleaning.extract_english_vocab.extract_english_words"]], "remove_rows_with_pattern() (in module soap_classification.data_cleaning.remove_undesired)": [[3, "soap_classification.data_cleaning.remove_undesired.remove_rows_with_pattern"]], "replace_sub() (in module soap_classification.data_cleaning.unify_syntax)": [[3, "soap_classification.data_cleaning.unify_syntax.replace_sub"]], "soap_classification.data_cleaning": [[3, "module-soap_classification.data_cleaning"]], "soap_classification.data_cleaning.extract_english_vocab": [[3, "module-soap_classification.data_cleaning.extract_english_vocab"]], "soap_classification.data_cleaning.only_soap": [[3, "module-soap_classification.data_cleaning.only_soap"]], "soap_classification.data_cleaning.regex": [[3, "module-soap_classification.data_cleaning.regex"]], "soap_classification.data_cleaning.remove_undesired": [[3, "module-soap_classification.data_cleaning.remove_undesired"]], "soap_classification.data_cleaning.retrieve_classified_soaps": [[3, "module-soap_classification.data_cleaning.retrieve_classified_soaps"]], "soap_classification.data_cleaning.total_soap": [[3, "module-soap_classification.data_cleaning.total_soap"]], "soap_classification.data_cleaning.unify_syntax": [[3, "module-soap_classification.data_cleaning.unify_syntax"]], "basictokenizer (class in soap_classification.preprocessing.tokenization_mod)": [[4, "soap_classification.preprocessing.tokenization_mod.BasicTokenizer"]], "fulltokenizer (class in soap_classification.preprocessing.tokenization_mod)": [[4, "soap_classification.preprocessing.tokenization_mod.FullTokenizer"]], "fulltokenizerformecab (class in soap_classification.preprocessing.tokenization_mod)": [[4, "soap_classification.preprocessing.tokenization_mod.FullTokenizerForMecab"]], "mecabtokenizer (class in soap_classification.preprocessing.tokenization_mod)": [[4, "soap_classification.preprocessing.tokenization_mod.MecabTokenizer"]], "wordpiecetokenizer (class in soap_classification.preprocessing.tokenization_mod)": [[4, "soap_classification.preprocessing.tokenization_mod.WordpieceTokenizer"]], "convert_by_vocab() (in module soap_classification.preprocessing.tokenization_mod)": [[4, "soap_classification.preprocessing.tokenization_mod.convert_by_vocab"]], "convert_ids_to_tokens() (in module soap_classification.preprocessing.tokenization_mod)": [[4, "soap_classification.preprocessing.tokenization_mod.convert_ids_to_tokens"]], "convert_ids_to_tokens() (soap_classification.preprocessing.tokenization_mod.fulltokenizer method)": [[4, "soap_classification.preprocessing.tokenization_mod.FullTokenizer.convert_ids_to_tokens"]], "convert_ids_to_tokens() (soap_classification.preprocessing.tokenization_mod.fulltokenizerformecab method)": [[4, "soap_classification.preprocessing.tokenization_mod.FullTokenizerForMecab.convert_ids_to_tokens"]], "convert_to_unicode() (in module soap_classification.preprocessing.tokenization_mod)": [[4, "soap_classification.preprocessing.tokenization_mod.convert_to_unicode"]], "convert_tokens_to_ids() (in module soap_classification.preprocessing.tokenization_mod)": [[4, "soap_classification.preprocessing.tokenization_mod.convert_tokens_to_ids"]], "convert_tokens_to_ids() (soap_classification.preprocessing.tokenization_mod.fulltokenizer method)": [[4, "soap_classification.preprocessing.tokenization_mod.FullTokenizer.convert_tokens_to_ids"]], "convert_tokens_to_ids() (soap_classification.preprocessing.tokenization_mod.fulltokenizerformecab method)": [[4, "soap_classification.preprocessing.tokenization_mod.FullTokenizerForMecab.convert_tokens_to_ids"]], "load_vocab() (in module soap_classification.preprocessing.tokenization_mod)": [[4, "soap_classification.preprocessing.tokenization_mod.load_vocab"]], "preprocess() (in module soap_classification.preprocessing.preprocess_text)": [[4, "soap_classification.preprocessing.preprocess_text.preprocess"]], "printable_text() (in module soap_classification.preprocessing.tokenization_mod)": [[4, "soap_classification.preprocessing.tokenization_mod.printable_text"]], "soap_classification.preprocessing": [[4, "module-soap_classification.preprocessing"]], "soap_classification.preprocessing.preprocess_text": [[4, "module-soap_classification.preprocessing.preprocess_text"]], "soap_classification.preprocessing.tokenization_mod": [[4, "module-soap_classification.preprocessing.tokenization_mod"]], "tokenize() (soap_classification.preprocessing.tokenization_mod.basictokenizer method)": [[4, "soap_classification.preprocessing.tokenization_mod.BasicTokenizer.tokenize"]], "tokenize() (soap_classification.preprocessing.tokenization_mod.fulltokenizer method)": [[4, "soap_classification.preprocessing.tokenization_mod.FullTokenizer.tokenize"]], "tokenize() (soap_classification.preprocessing.tokenization_mod.fulltokenizerformecab method)": [[4, "soap_classification.preprocessing.tokenization_mod.FullTokenizerForMecab.tokenize"]], "tokenize() (soap_classification.preprocessing.tokenization_mod.mecabtokenizer method)": [[4, "soap_classification.preprocessing.tokenization_mod.MecabTokenizer.tokenize"]], "tokenize() (soap_classification.preprocessing.tokenization_mod.wordpiecetokenizer method)": [[4, "soap_classification.preprocessing.tokenization_mod.WordpieceTokenizer.tokenize"]], "validate_case_matches_checkpoint() (in module soap_classification.preprocessing.tokenization_mod)": [[4, "soap_classification.preprocessing.tokenization_mod.validate_case_matches_checkpoint"]], "whitespace_tokenize() (in module soap_classification.preprocessing.tokenization_mod)": [[4, "soap_classification.preprocessing.tokenization_mod.whitespace_tokenize"]]}})