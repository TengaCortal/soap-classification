Search.setIndex({"docnames": ["index", "modules", "soap_classification", "soap_classification.data_construction", "soap_classification.prediction", "soap_classification.prediction.TF-IDF", "soap_classification.preprocessing"], "filenames": ["index.rst", "modules.rst", "soap_classification.rst", "soap_classification.data_construction.rst", "soap_classification.prediction.rst", "soap_classification.prediction.TF-IDF.rst", "soap_classification.preprocessing.rst"], "titles": ["Welcome to SOAP classification\u2019s documentation!", "soap_classification", "soap_classification package", "soap_classification.data_construction package", "soap_classification.prediction package", "soap_classification.prediction.TF-IDF package", "soap_classification.preprocessing package"], "terms": {"soap_classif": 0, "packag": [0, 1], "subpackag": [0, 1], "modul": [0, 1], "index": 0, "search": [0, 3], "page": 0, "data_construct": [1, 2], "submodul": [1, 2], "create_labeled_dataset": [1, 2], "extract_english_vocab": [1, 2], "only_soap": [1, 2], "regex": [1, 2], "remove_undesir": [1, 2], "retrieve_classified_soap": [1, 2], "split_soap_sect": [1, 2], "total_soap": [1, 2], "unify_syntax": [1, 2], "content": 1, "predict": [1, 2], "soap_partition": [1, 2], "preprocess": [1, 2], "hybrid_token": [1, 2], "preprocess_text": [1, 2], "tokenization_mod": [1, 2], "extract_english_word": [2, 3], "clean_text": [2, 3], "remove_rows_with_pattern": [2, 3], "create_section_csv": [2, 3], "extract_sect": [2, 3], "replace_sub": [2, 3], "tf": [2, 4, 6], "idf": [2, 4], "tfidf_classifi": [2, 4], "tfidf_evalu": [2, 4], "train_tfidf": [2, 4], "partition_all_soap_text": [2, 4], "partition_soap_text": [2, 4], "remove_annot": [2, 4], "basictoken": [2, 6], "token": [2, 6], "fulltoken": [2, 6], "convert_ids_to_token": [2, 6], "convert_tokens_to_id": [2, 6], "fulltokenizerformecab": [2, 6], "mecabtoken": [2, 6], "wordpiecetoken": [2, 6], "convert_by_vocab": [2, 6], "convert_to_unicod": [2, 6], "load_vocab": [2, 6], "printable_text": [2, 6], "validate_case_matches_checkpoint": [2, 6], "whitespace_token": [2, 6], "text": [3, 4, 6], "extract": [3, 4], "english": 3, "word": [3, 6], "from": [3, 4], "given": [3, 6], "paramet": [3, 6], "str": [3, 4, 6], "The": [3, 4, 6], "input": [3, 6], "contain": [3, 4], "return": [3, 4, 6], "A": [3, 6], "list": [3, 4, 6], "type": [3, 6], "clean": [3, 6], "remov": [3, 4], "unnecessari": 3, "charact": 3, "specifi": 3, "pattern": 3, "after": [3, 6], "datafram": [3, 4], "row": [3, 4], "panda": 3, "which": 3, "within": 3, "modifi": 3, "cleaned_classified_soaps_path": 3, "output_dir": 3, "creat": 3, "separ": [3, 4, 6], "csv": [3, 4], "file": [3, 4, 6], "each": 3, "section": 3, "sub": 3, "obj": 3, "asm": 3, "pln": 3, "classifi": 3, "soap": [3, 4], "note": [3, 4], "arg": [3, 4], "path": [3, 4], "directori": 3, "save": 3, "soaps_df": 3, "onli": 3, "replac": 3, "certain": 3, "substr": 3, "predefin": 3, "abbrevi": 3, "process": 3, "substitut": 3, "appli": [3, 6], "cleaned_soap": 4, "partit": 4, "all": 4, "output": [4, 6], "annot": 4, "everi": 4, "partitioned_soap": 4, "soap_text": 4, "us": [4, 6], "csv_file": 4, "variou": 6, "normal": 6, "format": 6, "step": 6, "class": 6, "do_lower_cas": 6, "fals": 6, "base": 6, "object": 6, "run": 6, "basic": 6, "punctuat": 6, "split": 6, "lower": 6, "case": 6, "etc": 6, "piec": 6, "vocab_fil": 6, "true": 6, "end": 6, "tokenzi": 6, "id": 6, "sub_token": 6, "mecab_ipadic_neologd": 6, "mecab_j_med": 6, "anonymize_person_nam": 6, "name_token": 6, "\uff4e": 6, "vocab": 6, "unk_token": 6, "unk": 6, "max_input_chars_per_word": 6, "200": 6, "wordpiec": 6, "its": 6, "thi": 6, "greedi": 6, "longest": 6, "match": 6, "first": 6, "algorithm": 6, "perform": 6, "vocabulari": 6, "For": 6, "exampl": 6, "unaff": 6, "un": 6, "aff": 6, "abl": 6, "singl": 6, "whitespac": 6, "should": 6, "have": 6, "alreadi": 6, "been": 6, "pass": 6, "through": 6, "item": 6, "convert": 6, "sequenc": 6, "inv_vocab": 6, "unicod": 6, "": 6, "assum": 6, "utf": 6, "8": 6, "load": 6, "dictionari": 6, "encod": 6, "wai": 6, "suitabl": 6, "print": 6, "log": 6, "init_checkpoint": 6, "check": 6, "whether": 6, "config": 6, "i": 6, "consist": 6, "checkpoint": 6, "name": 6}, "objects": {"": [[2, 0, 0, "-", "soap_classification"]], "soap_classification": [[3, 0, 0, "-", "data_construction"], [4, 0, 0, "-", "prediction"], [6, 0, 0, "-", "preprocessing"]], "soap_classification.data_construction": [[3, 0, 0, "-", "create_labeled_dataset"], [3, 0, 0, "-", "extract_english_vocab"], [3, 0, 0, "-", "only_soap"], [3, 0, 0, "-", "regex"], [3, 0, 0, "-", "remove_undesired"], [3, 0, 0, "-", "retrieve_classified_soaps"], [3, 0, 0, "-", "split_soap_sections"], [3, 0, 0, "-", "total_soap"], [3, 0, 0, "-", "unify_syntax"]], "soap_classification.data_construction.extract_english_vocab": [[3, 1, 1, "", "extract_english_words"]], "soap_classification.data_construction.remove_undesired": [[3, 1, 1, "", "clean_text"], [3, 1, 1, "", "remove_rows_with_pattern"]], "soap_classification.data_construction.split_soap_sections": [[3, 1, 1, "", "create_section_csv"], [3, 1, 1, "", "extract_section"]], "soap_classification.data_construction.unify_syntax": [[3, 1, 1, "", "replace_sub"]], "soap_classification.prediction": [[4, 0, 0, "-", "soap_partitioner"]], "soap_classification.prediction.soap_partitioner": [[4, 1, 1, "", "partition_all_soap_text"], [4, 1, 1, "", "partition_soap_text"], [4, 1, 1, "", "remove_annotations"]], "soap_classification.preprocessing": [[6, 0, 0, "-", "preprocess_text"], [6, 0, 0, "-", "tokenization_mod"]], "soap_classification.preprocessing.preprocess_text": [[6, 1, 1, "", "preprocess"]], "soap_classification.preprocessing.tokenization_mod": [[6, 2, 1, "", "BasicTokenizer"], [6, 2, 1, "", "FullTokenizer"], [6, 2, 1, "", "FullTokenizerForMecab"], [6, 2, 1, "", "MecabTokenizer"], [6, 2, 1, "", "WordpieceTokenizer"], [6, 1, 1, "", "convert_by_vocab"], [6, 1, 1, "", "convert_ids_to_tokens"], [6, 1, 1, "", "convert_to_unicode"], [6, 1, 1, "", "convert_tokens_to_ids"], [6, 1, 1, "", "load_vocab"], [6, 1, 1, "", "printable_text"], [6, 1, 1, "", "validate_case_matches_checkpoint"], [6, 1, 1, "", "whitespace_tokenize"]], "soap_classification.preprocessing.tokenization_mod.BasicTokenizer": [[6, 3, 1, "", "tokenize"]], "soap_classification.preprocessing.tokenization_mod.FullTokenizer": [[6, 3, 1, "", "convert_ids_to_tokens"], [6, 3, 1, "", "convert_tokens_to_ids"], [6, 3, 1, "", "tokenize"]], "soap_classification.preprocessing.tokenization_mod.FullTokenizerForMecab": [[6, 3, 1, "", "convert_ids_to_tokens"], [6, 3, 1, "", "convert_tokens_to_ids"], [6, 3, 1, "", "tokenize"]], "soap_classification.preprocessing.tokenization_mod.MecabTokenizer": [[6, 3, 1, "", "tokenize"]], "soap_classification.preprocessing.tokenization_mod.WordpieceTokenizer": [[6, 3, 1, "", "tokenize"]]}, "objtypes": {"0": "py:module", "1": "py:function", "2": "py:class", "3": "py:method"}, "objnames": {"0": ["py", "module", "Python module"], "1": ["py", "function", "Python function"], "2": ["py", "class", "Python class"], "3": ["py", "method", "Python method"]}, "titleterms": {"welcom": 0, "soap": 0, "classif": 0, "": 0, "document": 0, "content": [0, 2, 3, 4, 5, 6], "indic": 0, "tabl": 0, "soap_classif": [1, 2, 3, 4, 5, 6], "packag": [2, 3, 4, 5, 6], "subpackag": [2, 4], "modul": [2, 3, 4, 5, 6], "data_construct": 3, "submodul": [3, 4, 5, 6], "create_labeled_dataset": 3, "extract_english_vocab": 3, "only_soap": 3, "regex": 3, "remove_undesir": 3, "retrieve_classified_soap": 3, "split_soap_sect": 3, "total_soap": 3, "unify_syntax": 3, "predict": [4, 5], "soap_partition": 4, "tf": 5, "idf": 5, "tfidf_classifi": 5, "tfidf_evalu": 5, "train_tfidf": 5, "preprocess": 6, "hybrid_token": 6, "preprocess_text": 6, "tokenization_mod": 6}, "envversion": {"sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx": 60}, "alltitles": {"Welcome to SOAP classification\u2019s documentation!": [[0, "welcome-to-soap-classification-s-documentation"]], "Contents:": [[0, null]], "Indices and tables": [[0, "indices-and-tables"]], "soap_classification": [[1, "soap-classification"]], "soap_classification package": [[2, "soap-classification-package"]], "Subpackages": [[2, "subpackages"], [4, "subpackages"]], "Module contents": [[2, "module-soap_classification"], [3, "module-soap_classification.data_construction"], [4, "module-soap_classification.prediction"], [5, "module-contents"], [6, "module-soap_classification.preprocessing"]], "soap_classification.data_construction package": [[3, "soap-classification-data-construction-package"]], "Submodules": [[3, "submodules"], [4, "submodules"], [5, "submodules"], [6, "submodules"]], "soap_classification.data_construction.create_labeled_dataset module": [[3, "module-soap_classification.data_construction.create_labeled_dataset"]], "soap_classification.data_construction.extract_english_vocab module": [[3, "module-soap_classification.data_construction.extract_english_vocab"]], "soap_classification.data_construction.only_soap module": [[3, "module-soap_classification.data_construction.only_soap"]], "soap_classification.data_construction.regex module": [[3, "module-soap_classification.data_construction.regex"]], "soap_classification.data_construction.remove_undesired module": [[3, "module-soap_classification.data_construction.remove_undesired"]], "soap_classification.data_construction.retrieve_classified_soaps module": [[3, "module-soap_classification.data_construction.retrieve_classified_soaps"]], "soap_classification.data_construction.split_soap_sections module": [[3, "module-soap_classification.data_construction.split_soap_sections"]], "soap_classification.data_construction.total_soap module": [[3, "module-soap_classification.data_construction.total_soap"]], "soap_classification.data_construction.unify_syntax module": [[3, "module-soap_classification.data_construction.unify_syntax"]], "soap_classification.prediction package": [[4, "soap-classification-prediction-package"]], "soap_classification.prediction.soap_partitioner module": [[4, "module-soap_classification.prediction.soap_partitioner"]], "soap_classification.prediction.TF-IDF package": [[5, "soap-classification-prediction-tf-idf-package"]], "soap_classification.prediction.TF-IDF.tfidf_classifier module": [[5, "soap-classification-prediction-tf-idf-tfidf-classifier-module"]], "soap_classification.prediction.TF-IDF.tfidf_evaluation module": [[5, "soap-classification-prediction-tf-idf-tfidf-evaluation-module"]], "soap_classification.prediction.TF-IDF.train_tfidf module": [[5, "soap-classification-prediction-tf-idf-train-tfidf-module"]], "soap_classification.preprocessing package": [[6, "soap-classification-preprocessing-package"]], "soap_classification.preprocessing.hybrid_tokenization module": [[6, "soap-classification-preprocessing-hybrid-tokenization-module"]], "soap_classification.preprocessing.preprocess_text module": [[6, "module-soap_classification.preprocessing.preprocess_text"]], "soap_classification.preprocessing.tokenization_mod module": [[6, "module-soap_classification.preprocessing.tokenization_mod"]]}, "indexentries": {"module": [[2, "module-soap_classification"], [3, "module-soap_classification.data_construction"], [3, "module-soap_classification.data_construction.create_labeled_dataset"], [3, "module-soap_classification.data_construction.extract_english_vocab"], [3, "module-soap_classification.data_construction.only_soap"], [3, "module-soap_classification.data_construction.regex"], [3, "module-soap_classification.data_construction.remove_undesired"], [3, "module-soap_classification.data_construction.retrieve_classified_soaps"], [3, "module-soap_classification.data_construction.split_soap_sections"], [3, "module-soap_classification.data_construction.total_soap"], [3, "module-soap_classification.data_construction.unify_syntax"], [4, "module-soap_classification.prediction"], [4, "module-soap_classification.prediction.soap_partitioner"], [6, "module-soap_classification.preprocessing"], [6, "module-soap_classification.preprocessing.preprocess_text"], [6, "module-soap_classification.preprocessing.tokenization_mod"]], "soap_classification": [[2, "module-soap_classification"]], "clean_text() (in module soap_classification.data_construction.remove_undesired)": [[3, "soap_classification.data_construction.remove_undesired.clean_text"]], "create_section_csv() (in module soap_classification.data_construction.split_soap_sections)": [[3, "soap_classification.data_construction.split_soap_sections.create_section_csv"]], "extract_english_words() (in module soap_classification.data_construction.extract_english_vocab)": [[3, "soap_classification.data_construction.extract_english_vocab.extract_english_words"]], "extract_section() (in module soap_classification.data_construction.split_soap_sections)": [[3, "soap_classification.data_construction.split_soap_sections.extract_section"]], "remove_rows_with_pattern() (in module soap_classification.data_construction.remove_undesired)": [[3, "soap_classification.data_construction.remove_undesired.remove_rows_with_pattern"]], "replace_sub() (in module soap_classification.data_construction.unify_syntax)": [[3, "soap_classification.data_construction.unify_syntax.replace_sub"]], "soap_classification.data_construction": [[3, "module-soap_classification.data_construction"]], "soap_classification.data_construction.create_labeled_dataset": [[3, "module-soap_classification.data_construction.create_labeled_dataset"]], "soap_classification.data_construction.extract_english_vocab": [[3, "module-soap_classification.data_construction.extract_english_vocab"]], "soap_classification.data_construction.only_soap": [[3, "module-soap_classification.data_construction.only_soap"]], "soap_classification.data_construction.regex": [[3, "module-soap_classification.data_construction.regex"]], "soap_classification.data_construction.remove_undesired": [[3, "module-soap_classification.data_construction.remove_undesired"]], "soap_classification.data_construction.retrieve_classified_soaps": [[3, "module-soap_classification.data_construction.retrieve_classified_soaps"]], "soap_classification.data_construction.split_soap_sections": [[3, "module-soap_classification.data_construction.split_soap_sections"]], "soap_classification.data_construction.total_soap": [[3, "module-soap_classification.data_construction.total_soap"]], "soap_classification.data_construction.unify_syntax": [[3, "module-soap_classification.data_construction.unify_syntax"]], "partition_all_soap_text() (in module soap_classification.prediction.soap_partitioner)": [[4, "soap_classification.prediction.soap_partitioner.partition_all_soap_text"]], "partition_soap_text() (in module soap_classification.prediction.soap_partitioner)": [[4, "soap_classification.prediction.soap_partitioner.partition_soap_text"]], "remove_annotations() (in module soap_classification.prediction.soap_partitioner)": [[4, "soap_classification.prediction.soap_partitioner.remove_annotations"]], "soap_classification.prediction": [[4, "module-soap_classification.prediction"]], "soap_classification.prediction.soap_partitioner": [[4, "module-soap_classification.prediction.soap_partitioner"]], "basictokenizer (class in soap_classification.preprocessing.tokenization_mod)": [[6, "soap_classification.preprocessing.tokenization_mod.BasicTokenizer"]], "fulltokenizer (class in soap_classification.preprocessing.tokenization_mod)": [[6, "soap_classification.preprocessing.tokenization_mod.FullTokenizer"]], "fulltokenizerformecab (class in soap_classification.preprocessing.tokenization_mod)": [[6, "soap_classification.preprocessing.tokenization_mod.FullTokenizerForMecab"]], "mecabtokenizer (class in soap_classification.preprocessing.tokenization_mod)": [[6, "soap_classification.preprocessing.tokenization_mod.MecabTokenizer"]], "wordpiecetokenizer (class in soap_classification.preprocessing.tokenization_mod)": [[6, "soap_classification.preprocessing.tokenization_mod.WordpieceTokenizer"]], "convert_by_vocab() (in module soap_classification.preprocessing.tokenization_mod)": [[6, "soap_classification.preprocessing.tokenization_mod.convert_by_vocab"]], "convert_ids_to_tokens() (in module soap_classification.preprocessing.tokenization_mod)": [[6, "soap_classification.preprocessing.tokenization_mod.convert_ids_to_tokens"]], "convert_ids_to_tokens() (soap_classification.preprocessing.tokenization_mod.fulltokenizer method)": [[6, "soap_classification.preprocessing.tokenization_mod.FullTokenizer.convert_ids_to_tokens"]], "convert_ids_to_tokens() (soap_classification.preprocessing.tokenization_mod.fulltokenizerformecab method)": [[6, "soap_classification.preprocessing.tokenization_mod.FullTokenizerForMecab.convert_ids_to_tokens"]], "convert_to_unicode() (in module soap_classification.preprocessing.tokenization_mod)": [[6, "soap_classification.preprocessing.tokenization_mod.convert_to_unicode"]], "convert_tokens_to_ids() (in module soap_classification.preprocessing.tokenization_mod)": [[6, "soap_classification.preprocessing.tokenization_mod.convert_tokens_to_ids"]], "convert_tokens_to_ids() (soap_classification.preprocessing.tokenization_mod.fulltokenizer method)": [[6, "soap_classification.preprocessing.tokenization_mod.FullTokenizer.convert_tokens_to_ids"]], "convert_tokens_to_ids() (soap_classification.preprocessing.tokenization_mod.fulltokenizerformecab method)": [[6, "soap_classification.preprocessing.tokenization_mod.FullTokenizerForMecab.convert_tokens_to_ids"]], "load_vocab() (in module soap_classification.preprocessing.tokenization_mod)": [[6, "soap_classification.preprocessing.tokenization_mod.load_vocab"]], "preprocess() (in module soap_classification.preprocessing.preprocess_text)": [[6, "soap_classification.preprocessing.preprocess_text.preprocess"]], "printable_text() (in module soap_classification.preprocessing.tokenization_mod)": [[6, "soap_classification.preprocessing.tokenization_mod.printable_text"]], "soap_classification.preprocessing": [[6, "module-soap_classification.preprocessing"]], "soap_classification.preprocessing.preprocess_text": [[6, "module-soap_classification.preprocessing.preprocess_text"]], "soap_classification.preprocessing.tokenization_mod": [[6, "module-soap_classification.preprocessing.tokenization_mod"]], "tokenize() (soap_classification.preprocessing.tokenization_mod.basictokenizer method)": [[6, "soap_classification.preprocessing.tokenization_mod.BasicTokenizer.tokenize"]], "tokenize() (soap_classification.preprocessing.tokenization_mod.fulltokenizer method)": [[6, "soap_classification.preprocessing.tokenization_mod.FullTokenizer.tokenize"]], "tokenize() (soap_classification.preprocessing.tokenization_mod.fulltokenizerformecab method)": [[6, "soap_classification.preprocessing.tokenization_mod.FullTokenizerForMecab.tokenize"]], "tokenize() (soap_classification.preprocessing.tokenization_mod.mecabtokenizer method)": [[6, "soap_classification.preprocessing.tokenization_mod.MecabTokenizer.tokenize"]], "tokenize() (soap_classification.preprocessing.tokenization_mod.wordpiecetokenizer method)": [[6, "soap_classification.preprocessing.tokenization_mod.WordpieceTokenizer.tokenize"]], "validate_case_matches_checkpoint() (in module soap_classification.preprocessing.tokenization_mod)": [[6, "soap_classification.preprocessing.tokenization_mod.validate_case_matches_checkpoint"]], "whitespace_tokenize() (in module soap_classification.preprocessing.tokenization_mod)": [[6, "soap_classification.preprocessing.tokenization_mod.whitespace_tokenize"]]}})