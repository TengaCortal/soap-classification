Search.setIndex({"docnames": ["index", "modules", "soap_classification", "soap_classification.data_construction", "soap_classification.fairness", "soap_classification.prediction", "soap_classification.prediction.TF-IDF", "soap_classification.prediction.TFIDF", "soap_classification.prediction.Topic", "soap_classification.preprocessing"], "filenames": ["index.rst", "modules.rst", "soap_classification.rst", "soap_classification.data_construction.rst", "soap_classification.fairness.rst", "soap_classification.prediction.rst", "soap_classification.prediction.TF-IDF.rst", "soap_classification.prediction.TFIDF.rst", "soap_classification.prediction.Topic.rst", "soap_classification.preprocessing.rst"], "titles": ["Welcome to SOAP classification\u2019s documentation!", "soap_classification", "soap_classification package", "soap_classification.data_construction package", "soap_classification.fairness package", "soap_classification.prediction package", "soap_classification.prediction.TF-IDF package", "soap_classification.prediction.TFIDF package", "soap_classification.prediction.Topic package", "soap_classification.preprocessing package"], "terms": {"soap_classif": 0, "packag": [0, 1], "subpackag": [0, 1], "modul": [0, 1], "index": 0, "search": [0, 3, 4], "page": 0, "data_construct": [1, 2], "submodul": [1, 2], "create_labeled_dataset": [1, 2], "extract_english_vocab": [1, 2], "only_soap": [1, 2], "regex": [1, 2], "remove_undesir": [1, 2], "retrieve_classified_soap": [1, 2], "split_soap_sect": [1, 2], "total_soap": [1, 2], "unify_syntax": [1, 2], "content": 1, "fair": [1, 2], "data": [1, 2, 7], "evaluate_fair": [1, 2], "predict": [1, 2, 4], "soap_partition": [1, 2], "preprocess": [1, 2], "hybrid_token": [1, 2], "preprocess_text": [1, 2], "tokenization_mod": [1, 2], "extract_english_word": [2, 3], "clean_text": [2, 3, 4], "remove_rows_with_pattern": [2, 3, 4], "create_section_csv": [2, 3, 4], "extract_sect": [2, 3, 4], "replace_sub": [2, 3, 4], "extract_nam": [2, 4], "map_age_to_label": [2, 4], "tfidf": [2, 5], "tfidf_classifi": [2, 5], "tfidf_evalu": [2, 5], "tfidf_svd_cosin": [2, 5], "train_tfidf": [2, 5], "topic": [2, 5], "partition_all_soap_text": [2, 5], "partition_soap_text": [2, 5], "remove_annot": [2, 5], "basictoken": [2, 9], "token": [2, 9], "fulltoken": [2, 9], "convert_ids_to_token": [2, 9], "convert_tokens_to_id": [2, 9], "fulltokenizerformecab": [2, 9], "mecabtoken": [2, 9], "wordpiecetoken": [2, 9], "convert_by_vocab": [2, 9], "convert_to_unicod": [2, 9], "load_vocab": [2, 9], "printable_text": [2, 9], "validate_case_matches_checkpoint": [2, 9], "whitespace_token": [2, 9], "text": [3, 4, 5, 9], "extract": [3, 4, 5], "english": 3, "word": [3, 9], "from": [3, 4, 5], "given": [3, 9], "paramet": [3, 4, 7, 9], "str": [3, 4, 5, 7, 9], "The": [3, 4, 5, 7, 9], "input": [3, 4, 9], "contain": [3, 4, 5, 7], "return": [3, 4, 5, 7, 9], "A": [3, 4, 7, 9], "list": [3, 5, 9], "type": [3, 4, 7, 9], "clean": [3, 4, 9], "remov": [3, 4, 5], "unnecessari": [3, 4], "charact": [3, 4], "specifi": [3, 4], "pattern": [3, 4], "after": [3, 4, 9], "datafram": [3, 4, 5], "row": [3, 4, 5], "panda": [3, 4], "which": [3, 4], "within": [3, 4], "modifi": [3, 4], "cleaned_classified_soaps_path": [3, 4], "output_dir": [3, 4], "creat": [3, 4], "separ": [3, 4, 5, 9], "csv": [3, 4, 5, 7], "file": [3, 4, 5, 7, 9], "each": [3, 4, 7], "section": [3, 4], "sub": [3, 4], "obj": [3, 4], "asm": [3, 4], "pln": [3, 4], "classifi": [3, 4, 7], "soap": [3, 4, 5], "note": [3, 4, 5], "arg": [3, 4, 5, 7], "path": [3, 4, 5, 7], "directori": [3, 4], "save": [3, 4], "soaps_df": [3, 4], "onli": [3, 4], "replac": [3, 4], "certain": [3, 4], "substr": [3, 4], "predefin": [3, 4], "abbrevi": [3, 4], "process": [3, 4], "substitut": [3, 4], "appli": [3, 4, 9], "cleaned_classified_fairness_path": 4, "d": 4, "name": [4, 9], "dictionari": [4, 7, 9], "string": 4, "fairness_df": 4, "ag": 4, "map": 4, "rang": 4, "label": [4, 7], "int": [4, 7], "correspond": 4, "kei": 4, "subgroup_column": 4, "evalu": 4, "dataset": [4, 7], "column": 4, "subgroup": 4, "inform": 4, "tupl": 4, "percentag": 4, "correct": 4, "count": 4, "classify_with_cosine_similar": [5, 7], "train_tfidf_classifi": [5, 7], "cleaned_soap": 5, "partit": 5, "all": 5, "output": [5, 9], "annot": 5, "everi": 5, "partitioned_soap": 5, "soap_text": 5, "us": [5, 7, 9], "csv_file": 5, "queri": 7, "x_train_tfidf_reduc": 7, "y_train": 7, "vector": 7, "k": 7, "5": 7, "cosin": 7, "similar": 7, "arrai": 7, "like": 7, "train": 7, "tf": [7, 9], "idf": 7, "matrix": 7, "reduc": 7, "dimens": 7, "transform": 7, "option": 7, "number": 7, "nearest": 7, "neighbor": 7, "consid": 7, "default": 7, "normal": [7, 9], "vote": 7, "class": [7, 9], "base": [7, 9], "dict": 7, "dataset_fil": 7, "provid": 7, "clf": 7, "pipelin": 7, "classif": 7, "variou": 9, "format": 9, "step": 9, "do_lower_cas": 9, "fals": 9, "object": 9, "run": 9, "basic": 9, "punctuat": 9, "split": 9, "lower": 9, "case": 9, "etc": 9, "piec": 9, "vocab_fil": 9, "true": 9, "end": 9, "tokenzi": 9, "id": 9, "sub_token": 9, "mecab_ipadic_neologd": 9, "mecab_j_med": 9, "anonymize_person_nam": 9, "name_token": 9, "\uff4e": 9, "vocab": 9, "unk_token": 9, "unk": 9, "max_input_chars_per_word": 9, "200": 9, "wordpiec": 9, "its": 9, "thi": 9, "greedi": 9, "longest": 9, "match": 9, "first": 9, "algorithm": 9, "perform": 9, "vocabulari": 9, "For": 9, "exampl": 9, "unaff": 9, "un": 9, "aff": 9, "abl": 9, "singl": 9, "whitespac": 9, "should": 9, "have": 9, "alreadi": 9, "been": 9, "pass": 9, "through": 9, "item": 9, "convert": 9, "sequenc": 9, "inv_vocab": 9, "unicod": 9, "": 9, "assum": 9, "utf": 9, "8": 9, "load": 9, "encod": 9, "wai": 9, "suitabl": 9, "print": 9, "log": 9, "init_checkpoint": 9, "check": 9, "whether": 9, "config": 9, "i": 9, "consist": 9, "checkpoint": 9}, "objects": {"": [[2, 0, 0, "-", "soap_classification"]], "soap_classification": [[3, 0, 0, "-", "data_construction"], [4, 0, 0, "-", "fairness"], [5, 0, 0, "-", "prediction"], [9, 0, 0, "-", "preprocessing"]], "soap_classification.data_construction": [[3, 0, 0, "-", "create_labeled_dataset"], [3, 0, 0, "-", "extract_english_vocab"], [3, 0, 0, "-", "only_soap"], [3, 0, 0, "-", "regex"], [3, 0, 0, "-", "remove_undesired"], [3, 0, 0, "-", "retrieve_classified_soaps"], [3, 0, 0, "-", "split_soap_sections"], [3, 0, 0, "-", "total_soap"], [3, 0, 0, "-", "unify_syntax"]], "soap_classification.data_construction.extract_english_vocab": [[3, 1, 1, "", "extract_english_words"]], "soap_classification.data_construction.remove_undesired": [[3, 1, 1, "", "clean_text"], [3, 1, 1, "", "remove_rows_with_pattern"]], "soap_classification.data_construction.split_soap_sections": [[3, 1, 1, "", "create_section_csv"], [3, 1, 1, "", "extract_section"]], "soap_classification.data_construction.unify_syntax": [[3, 1, 1, "", "replace_sub"]], "soap_classification.fairness": [[4, 0, 0, "-", "data"], [4, 0, 0, "-", "evaluate_fairness"]], "soap_classification.fairness.data": [[4, 1, 1, "", "clean_text"], [4, 1, 1, "", "create_section_csv"], [4, 1, 1, "", "extract_name"], [4, 1, 1, "", "extract_section"], [4, 1, 1, "", "map_age_to_label"], [4, 1, 1, "", "remove_rows_with_pattern"], [4, 1, 1, "", "replace_sub"]], "soap_classification.fairness.evaluate_fairness": [[4, 1, 1, "", "evaluate_fairness"]], "soap_classification.prediction": [[7, 0, 0, "-", "TFIDF"], [8, 0, 0, "-", "Topic"], [5, 0, 0, "-", "soap_partitioner"]], "soap_classification.prediction.TFIDF": [[7, 0, 0, "-", "tfidf_evaluation"], [7, 0, 0, "-", "tfidf_svd_cosine"], [7, 0, 0, "-", "train_tfidf"]], "soap_classification.prediction.TFIDF.tfidf_svd_cosine": [[7, 1, 1, "", "classify_with_cosine_similarity"]], "soap_classification.prediction.TFIDF.train_tfidf": [[7, 1, 1, "", "train_tfidf_classifier"]], "soap_classification.prediction.soap_partitioner": [[5, 1, 1, "", "partition_all_soap_text"], [5, 1, 1, "", "partition_soap_text"], [5, 1, 1, "", "remove_annotations"]], "soap_classification.preprocessing": [[9, 0, 0, "-", "preprocess_text"], [9, 0, 0, "-", "tokenization_mod"]], "soap_classification.preprocessing.preprocess_text": [[9, 1, 1, "", "preprocess"]], "soap_classification.preprocessing.tokenization_mod": [[9, 2, 1, "", "BasicTokenizer"], [9, 2, 1, "", "FullTokenizer"], [9, 2, 1, "", "FullTokenizerForMecab"], [9, 2, 1, "", "MecabTokenizer"], [9, 2, 1, "", "WordpieceTokenizer"], [9, 1, 1, "", "convert_by_vocab"], [9, 1, 1, "", "convert_ids_to_tokens"], [9, 1, 1, "", "convert_to_unicode"], [9, 1, 1, "", "convert_tokens_to_ids"], [9, 1, 1, "", "load_vocab"], [9, 1, 1, "", "printable_text"], [9, 1, 1, "", "validate_case_matches_checkpoint"], [9, 1, 1, "", "whitespace_tokenize"]], "soap_classification.preprocessing.tokenization_mod.BasicTokenizer": [[9, 3, 1, "", "tokenize"]], "soap_classification.preprocessing.tokenization_mod.FullTokenizer": [[9, 3, 1, "", "convert_ids_to_tokens"], [9, 3, 1, "", "convert_tokens_to_ids"], [9, 3, 1, "", "tokenize"]], "soap_classification.preprocessing.tokenization_mod.FullTokenizerForMecab": [[9, 3, 1, "", "convert_ids_to_tokens"], [9, 3, 1, "", "convert_tokens_to_ids"], [9, 3, 1, "", "tokenize"]], "soap_classification.preprocessing.tokenization_mod.MecabTokenizer": [[9, 3, 1, "", "tokenize"]], "soap_classification.preprocessing.tokenization_mod.WordpieceTokenizer": [[9, 3, 1, "", "tokenize"]]}, "objtypes": {"0": "py:module", "1": "py:function", "2": "py:class", "3": "py:method"}, "objnames": {"0": ["py", "module", "Python module"], "1": ["py", "function", "Python function"], "2": ["py", "class", "Python class"], "3": ["py", "method", "Python method"]}, "titleterms": {"welcom": 0, "soap": 0, "classif": 0, "": 0, "document": 0, "content": [0, 2, 3, 4, 5, 6, 7, 8, 9], "indic": 0, "tabl": 0, "soap_classif": [1, 2, 3, 4, 5, 6, 7, 8, 9], "packag": [2, 3, 4, 5, 6, 7, 8, 9], "subpackag": [2, 5], "modul": [2, 3, 4, 5, 6, 7, 8, 9], "data_construct": 3, "submodul": [3, 4, 5, 6, 7, 8, 9], "create_labeled_dataset": 3, "extract_english_vocab": 3, "only_soap": 3, "regex": 3, "remove_undesir": 3, "retrieve_classified_soap": 3, "split_soap_sect": 3, "total_soap": 3, "unify_syntax": 3, "fair": 4, "data": 4, "evaluate_fair": 4, "predict": [5, 6, 7, 8], "soap_partition": 5, "tf": 6, "idf": 6, "tfidf_classifi": [6, 7], "tfidf_evalu": [6, 7], "train_tfidf": [6, 7], "tfidf": 7, "tfidf_svd_cosin": 7, "topic": 8, "preprocess": 9, "hybrid_token": 9, "preprocess_text": 9, "tokenization_mod": 9}, "envversion": {"sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx": 60}, "alltitles": {"Welcome to SOAP classification\u2019s documentation!": [[0, "welcome-to-soap-classification-s-documentation"]], "Contents:": [[0, null]], "Indices and tables": [[0, "indices-and-tables"]], "soap_classification": [[1, "soap-classification"]], "soap_classification package": [[2, "soap-classification-package"]], "Subpackages": [[2, "subpackages"], [5, "subpackages"]], "Module contents": [[2, "module-soap_classification"], [3, "module-soap_classification.data_construction"], [4, "module-soap_classification.fairness"], [5, "module-soap_classification.prediction"], [6, "module-contents"], [7, "module-soap_classification.prediction.TFIDF"], [8, "module-soap_classification.prediction.Topic"], [9, "module-soap_classification.preprocessing"]], "soap_classification.data_construction package": [[3, "soap-classification-data-construction-package"]], "Submodules": [[3, "submodules"], [4, "submodules"], [5, "submodules"], [6, "submodules"], [7, "submodules"], [8, "submodules"], [9, "submodules"]], "soap_classification.data_construction.create_labeled_dataset module": [[3, "module-soap_classification.data_construction.create_labeled_dataset"]], "soap_classification.data_construction.extract_english_vocab module": [[3, "module-soap_classification.data_construction.extract_english_vocab"]], "soap_classification.data_construction.only_soap module": [[3, "module-soap_classification.data_construction.only_soap"]], "soap_classification.data_construction.regex module": [[3, "module-soap_classification.data_construction.regex"]], "soap_classification.data_construction.remove_undesired module": [[3, "module-soap_classification.data_construction.remove_undesired"]], "soap_classification.data_construction.retrieve_classified_soaps module": [[3, "module-soap_classification.data_construction.retrieve_classified_soaps"]], "soap_classification.data_construction.split_soap_sections module": [[3, "module-soap_classification.data_construction.split_soap_sections"]], "soap_classification.data_construction.total_soap module": [[3, "module-soap_classification.data_construction.total_soap"]], "soap_classification.data_construction.unify_syntax module": [[3, "module-soap_classification.data_construction.unify_syntax"]], "soap_classification.fairness package": [[4, "soap-classification-fairness-package"]], "soap_classification.fairness.data module": [[4, "module-soap_classification.fairness.data"]], "soap_classification.fairness.evaluate_fairness module": [[4, "module-soap_classification.fairness.evaluate_fairness"]], "soap_classification.prediction package": [[5, "soap-classification-prediction-package"]], "soap_classification.prediction.soap_partitioner module": [[5, "module-soap_classification.prediction.soap_partitioner"]], "soap_classification.prediction.TF-IDF package": [[6, "soap-classification-prediction-tf-idf-package"]], "soap_classification.prediction.TF-IDF.tfidf_classifier module": [[6, "soap-classification-prediction-tf-idf-tfidf-classifier-module"]], "soap_classification.prediction.TF-IDF.tfidf_evaluation module": [[6, "soap-classification-prediction-tf-idf-tfidf-evaluation-module"]], "soap_classification.prediction.TF-IDF.train_tfidf module": [[6, "soap-classification-prediction-tf-idf-train-tfidf-module"]], "soap_classification.prediction.TFIDF package": [[7, "soap-classification-prediction-tfidf-package"]], "soap_classification.prediction.TFIDF.tfidf_classifier module": [[7, "soap-classification-prediction-tfidf-tfidf-classifier-module"]], "soap_classification.prediction.TFIDF.tfidf_evaluation module": [[7, "module-soap_classification.prediction.TFIDF.tfidf_evaluation"]], "soap_classification.prediction.TFIDF.tfidf_svd_cosine module": [[7, "module-soap_classification.prediction.TFIDF.tfidf_svd_cosine"]], "soap_classification.prediction.TFIDF.train_tfidf module": [[7, "module-soap_classification.prediction.TFIDF.train_tfidf"]], "soap_classification.prediction.Topic package": [[8, "soap-classification-prediction-topic-package"]], "soap_classification.prediction.Topic.topic module": [[8, "soap-classification-prediction-topic-topic-module"]], "soap_classification.preprocessing package": [[9, "soap-classification-preprocessing-package"]], "soap_classification.preprocessing.hybrid_tokenization module": [[9, "soap-classification-preprocessing-hybrid-tokenization-module"]], "soap_classification.preprocessing.preprocess_text module": [[9, "module-soap_classification.preprocessing.preprocess_text"]], "soap_classification.preprocessing.tokenization_mod module": [[9, "module-soap_classification.preprocessing.tokenization_mod"]]}, "indexentries": {"module": [[2, "module-soap_classification"], [3, "module-soap_classification.data_construction"], [3, "module-soap_classification.data_construction.create_labeled_dataset"], [3, "module-soap_classification.data_construction.extract_english_vocab"], [3, "module-soap_classification.data_construction.only_soap"], [3, "module-soap_classification.data_construction.regex"], [3, "module-soap_classification.data_construction.remove_undesired"], [3, "module-soap_classification.data_construction.retrieve_classified_soaps"], [3, "module-soap_classification.data_construction.split_soap_sections"], [3, "module-soap_classification.data_construction.total_soap"], [3, "module-soap_classification.data_construction.unify_syntax"], [4, "module-soap_classification.fairness"], [4, "module-soap_classification.fairness.data"], [4, "module-soap_classification.fairness.evaluate_fairness"], [5, "module-soap_classification.prediction"], [5, "module-soap_classification.prediction.soap_partitioner"], [7, "module-soap_classification.prediction.TFIDF"], [7, "module-soap_classification.prediction.TFIDF.tfidf_evaluation"], [7, "module-soap_classification.prediction.TFIDF.tfidf_svd_cosine"], [7, "module-soap_classification.prediction.TFIDF.train_tfidf"], [8, "module-soap_classification.prediction.Topic"], [9, "module-soap_classification.preprocessing"], [9, "module-soap_classification.preprocessing.preprocess_text"], [9, "module-soap_classification.preprocessing.tokenization_mod"]], "soap_classification": [[2, "module-soap_classification"]], "clean_text() (in module soap_classification.data_construction.remove_undesired)": [[3, "soap_classification.data_construction.remove_undesired.clean_text"]], "create_section_csv() (in module soap_classification.data_construction.split_soap_sections)": [[3, "soap_classification.data_construction.split_soap_sections.create_section_csv"]], "extract_english_words() (in module soap_classification.data_construction.extract_english_vocab)": [[3, "soap_classification.data_construction.extract_english_vocab.extract_english_words"]], "extract_section() (in module soap_classification.data_construction.split_soap_sections)": [[3, "soap_classification.data_construction.split_soap_sections.extract_section"]], "remove_rows_with_pattern() (in module soap_classification.data_construction.remove_undesired)": [[3, "soap_classification.data_construction.remove_undesired.remove_rows_with_pattern"]], "replace_sub() (in module soap_classification.data_construction.unify_syntax)": [[3, "soap_classification.data_construction.unify_syntax.replace_sub"]], "soap_classification.data_construction": [[3, "module-soap_classification.data_construction"]], "soap_classification.data_construction.create_labeled_dataset": [[3, "module-soap_classification.data_construction.create_labeled_dataset"]], "soap_classification.data_construction.extract_english_vocab": [[3, "module-soap_classification.data_construction.extract_english_vocab"]], "soap_classification.data_construction.only_soap": [[3, "module-soap_classification.data_construction.only_soap"]], "soap_classification.data_construction.regex": [[3, "module-soap_classification.data_construction.regex"]], "soap_classification.data_construction.remove_undesired": [[3, "module-soap_classification.data_construction.remove_undesired"]], "soap_classification.data_construction.retrieve_classified_soaps": [[3, "module-soap_classification.data_construction.retrieve_classified_soaps"]], "soap_classification.data_construction.split_soap_sections": [[3, "module-soap_classification.data_construction.split_soap_sections"]], "soap_classification.data_construction.total_soap": [[3, "module-soap_classification.data_construction.total_soap"]], "soap_classification.data_construction.unify_syntax": [[3, "module-soap_classification.data_construction.unify_syntax"]], "clean_text() (in module soap_classification.fairness.data)": [[4, "soap_classification.fairness.data.clean_text"]], "create_section_csv() (in module soap_classification.fairness.data)": [[4, "soap_classification.fairness.data.create_section_csv"]], "evaluate_fairness() (in module soap_classification.fairness.evaluate_fairness)": [[4, "soap_classification.fairness.evaluate_fairness.evaluate_fairness"]], "extract_name() (in module soap_classification.fairness.data)": [[4, "soap_classification.fairness.data.extract_name"]], "extract_section() (in module soap_classification.fairness.data)": [[4, "soap_classification.fairness.data.extract_section"]], "map_age_to_label() (in module soap_classification.fairness.data)": [[4, "soap_classification.fairness.data.map_age_to_label"]], "remove_rows_with_pattern() (in module soap_classification.fairness.data)": [[4, "soap_classification.fairness.data.remove_rows_with_pattern"]], "replace_sub() (in module soap_classification.fairness.data)": [[4, "soap_classification.fairness.data.replace_sub"]], "soap_classification.fairness": [[4, "module-soap_classification.fairness"]], "soap_classification.fairness.data": [[4, "module-soap_classification.fairness.data"]], "soap_classification.fairness.evaluate_fairness": [[4, "module-soap_classification.fairness.evaluate_fairness"]], "partition_all_soap_text() (in module soap_classification.prediction.soap_partitioner)": [[5, "soap_classification.prediction.soap_partitioner.partition_all_soap_text"]], "partition_soap_text() (in module soap_classification.prediction.soap_partitioner)": [[5, "soap_classification.prediction.soap_partitioner.partition_soap_text"]], "remove_annotations() (in module soap_classification.prediction.soap_partitioner)": [[5, "soap_classification.prediction.soap_partitioner.remove_annotations"]], "soap_classification.prediction": [[5, "module-soap_classification.prediction"]], "soap_classification.prediction.soap_partitioner": [[5, "module-soap_classification.prediction.soap_partitioner"]], "classify_with_cosine_similarity() (in module soap_classification.prediction.tfidf.tfidf_svd_cosine)": [[7, "soap_classification.prediction.TFIDF.tfidf_svd_cosine.classify_with_cosine_similarity"]], "soap_classification.prediction.tfidf": [[7, "module-soap_classification.prediction.TFIDF"]], "soap_classification.prediction.tfidf.tfidf_evaluation": [[7, "module-soap_classification.prediction.TFIDF.tfidf_evaluation"]], "soap_classification.prediction.tfidf.tfidf_svd_cosine": [[7, "module-soap_classification.prediction.TFIDF.tfidf_svd_cosine"]], "soap_classification.prediction.tfidf.train_tfidf": [[7, "module-soap_classification.prediction.TFIDF.train_tfidf"]], "train_tfidf_classifier() (in module soap_classification.prediction.tfidf.train_tfidf)": [[7, "soap_classification.prediction.TFIDF.train_tfidf.train_tfidf_classifier"]], "soap_classification.prediction.topic": [[8, "module-soap_classification.prediction.Topic"]], "basictokenizer (class in soap_classification.preprocessing.tokenization_mod)": [[9, "soap_classification.preprocessing.tokenization_mod.BasicTokenizer"]], "fulltokenizer (class in soap_classification.preprocessing.tokenization_mod)": [[9, "soap_classification.preprocessing.tokenization_mod.FullTokenizer"]], "fulltokenizerformecab (class in soap_classification.preprocessing.tokenization_mod)": [[9, "soap_classification.preprocessing.tokenization_mod.FullTokenizerForMecab"]], "mecabtokenizer (class in soap_classification.preprocessing.tokenization_mod)": [[9, "soap_classification.preprocessing.tokenization_mod.MecabTokenizer"]], "wordpiecetokenizer (class in soap_classification.preprocessing.tokenization_mod)": [[9, "soap_classification.preprocessing.tokenization_mod.WordpieceTokenizer"]], "convert_by_vocab() (in module soap_classification.preprocessing.tokenization_mod)": [[9, "soap_classification.preprocessing.tokenization_mod.convert_by_vocab"]], "convert_ids_to_tokens() (in module soap_classification.preprocessing.tokenization_mod)": [[9, "soap_classification.preprocessing.tokenization_mod.convert_ids_to_tokens"]], "convert_ids_to_tokens() (soap_classification.preprocessing.tokenization_mod.fulltokenizer method)": [[9, "soap_classification.preprocessing.tokenization_mod.FullTokenizer.convert_ids_to_tokens"]], "convert_ids_to_tokens() (soap_classification.preprocessing.tokenization_mod.fulltokenizerformecab method)": [[9, "soap_classification.preprocessing.tokenization_mod.FullTokenizerForMecab.convert_ids_to_tokens"]], "convert_to_unicode() (in module soap_classification.preprocessing.tokenization_mod)": [[9, "soap_classification.preprocessing.tokenization_mod.convert_to_unicode"]], "convert_tokens_to_ids() (in module soap_classification.preprocessing.tokenization_mod)": [[9, "soap_classification.preprocessing.tokenization_mod.convert_tokens_to_ids"]], "convert_tokens_to_ids() (soap_classification.preprocessing.tokenization_mod.fulltokenizer method)": [[9, "soap_classification.preprocessing.tokenization_mod.FullTokenizer.convert_tokens_to_ids"]], "convert_tokens_to_ids() (soap_classification.preprocessing.tokenization_mod.fulltokenizerformecab method)": [[9, "soap_classification.preprocessing.tokenization_mod.FullTokenizerForMecab.convert_tokens_to_ids"]], "load_vocab() (in module soap_classification.preprocessing.tokenization_mod)": [[9, "soap_classification.preprocessing.tokenization_mod.load_vocab"]], "preprocess() (in module soap_classification.preprocessing.preprocess_text)": [[9, "soap_classification.preprocessing.preprocess_text.preprocess"]], "printable_text() (in module soap_classification.preprocessing.tokenization_mod)": [[9, "soap_classification.preprocessing.tokenization_mod.printable_text"]], "soap_classification.preprocessing": [[9, "module-soap_classification.preprocessing"]], "soap_classification.preprocessing.preprocess_text": [[9, "module-soap_classification.preprocessing.preprocess_text"]], "soap_classification.preprocessing.tokenization_mod": [[9, "module-soap_classification.preprocessing.tokenization_mod"]], "tokenize() (soap_classification.preprocessing.tokenization_mod.basictokenizer method)": [[9, "soap_classification.preprocessing.tokenization_mod.BasicTokenizer.tokenize"]], "tokenize() (soap_classification.preprocessing.tokenization_mod.fulltokenizer method)": [[9, "soap_classification.preprocessing.tokenization_mod.FullTokenizer.tokenize"]], "tokenize() (soap_classification.preprocessing.tokenization_mod.fulltokenizerformecab method)": [[9, "soap_classification.preprocessing.tokenization_mod.FullTokenizerForMecab.tokenize"]], "tokenize() (soap_classification.preprocessing.tokenization_mod.mecabtokenizer method)": [[9, "soap_classification.preprocessing.tokenization_mod.MecabTokenizer.tokenize"]], "tokenize() (soap_classification.preprocessing.tokenization_mod.wordpiecetokenizer method)": [[9, "soap_classification.preprocessing.tokenization_mod.WordpieceTokenizer.tokenize"]], "validate_case_matches_checkpoint() (in module soap_classification.preprocessing.tokenization_mod)": [[9, "soap_classification.preprocessing.tokenization_mod.validate_case_matches_checkpoint"]], "whitespace_tokenize() (in module soap_classification.preprocessing.tokenization_mod)": [[9, "soap_classification.preprocessing.tokenization_mod.whitespace_tokenize"]]}})